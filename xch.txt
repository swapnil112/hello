from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix

# Dummy data
y_true = [1, 0, 1, 1, 0, 1, 0, 1, 0, 0]
y_pred = [1, 1, 1, 0, 0, 1, 0, 0, 1, 0]

# F1-score (macro)
f1_macro = f1_score(y_true, y_pred, average='macro')

# F1-score (weighted)
f1_weighted = f1_score(y_true, y_pred, average='weighted')

# Support
support = confusion_matrix(y_true, y_pred)

# Precision
precision = precision_score(y_true, y_pred)

# Recall
recall = recall_score(y_true, y_pred)

# TP, FP, TN, FN
tp, fp, tn, fn = confusion_matrix(y_true, y_pred).ravel()

print("F1-score (macro):", f1_macro)
print("F1-score (weighted):", f1_weighted)
print("Support:\n", support)
print("Precision:", precision)
print("Recall:", recall)
print("TP:", tp)
print("FP:", fp)
print("TN:", tn)
print("FN:", fn)


from sklearn.metrics import classification_report

# Dummy data
y_true = [1, 0, 1, 1, 0, 1, 0, 1, 0, 0]
y_pred = [1, 1, 1, 0, 0, 1, 0, 0, 1, 0]

# Classification report
class_report = classification_report(y_true, y_pred, output_dict=True)

# Print support for each class
for class_label, metrics in class_report.items():
    print("Class:", class_label)
    print("Support:", metrics["support"])
