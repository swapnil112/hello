from sklearn.metrics import confusion_matrix

y_true = [1, 0, 1, 0, 1, 1, 0, 0]
y_pred = [1, 1, 0, 0, 1, 0, 1, 0]

# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Extract individual level-wise counts
tn_0, fp_0, fn_0, tp_0 = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]
tn_1, fp_1, fn_1, tp_1 = cm[1, 1], cm[1, 0], cm[0, 1], cm[0, 0]

print("Class 0")
print("TP:", tp_0)
print("FP:", fp_0)
print("TN:", tn_0)
print("FN:", fn_0)

print("Class 1")
print("TP:", tp_1)
print("FP:", fp_1)
print("TN:", tn_1)
print("FN:", fn_1)


# Import required libraries
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
dataset = pd.read_csv('your_dataset.csv')

# Separate features and target variable
X = dataset.drop('target_variable', axis=1)
y = dataset['target_variable']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the XGBoost Classifier model
xgb_clf = xgb.XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100, objective='binary:logistic', seed=42)

# Fit the model on the training data
xgb_clf.fit(X_train, y_train)

# Predict on the test data
y_pred = xgb_clf.predict(X_test)

# Calculate accuracy score of the model
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy score
print("Accuracy: %.2f%%" % (accuracy * 100.0))
