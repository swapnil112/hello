from transformers import BertTokenizer, BertModel
import torch
from gensim.models import Word2Vec
import numpy as np

# Load the BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)

# Load the Word2Vec model
word2vec_model = Word2Vec.load('path/to/word2vec/model')

# Define the OOV token
oov_token = np.zeros((word2vec_model.vector_size,))

# Tokenize a sentence
sentence = "What is the capital of France?"
tokens = tokenizer.tokenize(sentence)

# Get the BERT embeddings
input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)
outputs = model(input_ids)
hidden_states = outputs[2]
last_four_layers = [hidden_states[i] for i in range(-4, 0)]
sentence_embedding = torch.mean(torch.stack(last_four_layers), dim=0).squeeze()
bert_embeddings = sentence_embedding.detach().numpy()

# Compute the Word2Vec embeddings
word2vec_embeddings = []
for token in tokens:
    if token in word2vec_model.wv.vocab:
        word2vec_embeddings.append(word2vec_model.wv[token])
    else:
        word2vec_embeddings.append(oov_token)

# Combine the BERT and Word2Vec embeddings
combined_embeddings = np.concatenate((bert_embeddings, np.mean(word2vec_embeddings, axis=0)))
