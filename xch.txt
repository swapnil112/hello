from transformers import BertTokenizer, BertModel
import torch
from gensim.models import Word2Vec
import numpy as np

# Load the BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)

# Load the Word2Vec model
word2vec_model = Word2Vec.load('path/to/word2vec/model')

# Tokenize a sentence
sentence = "The quick brown fox jumps over the lazy dog"
tokens = tokenizer.tokenize(sentence)

# Get the BERT embeddings
input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)
outputs = model(input_ids)
hidden_states = outputs[2]
last_four_layers = [hidden_states[i] for i in range(-4, 0)]
sentence_embedding = torch.mean(torch.stack(last_four_layers), dim=0).squeeze()
bert_embeddings = sentence_embedding.detach().numpy()

# Compute the Word2Vec embeddings
print(tokens)
print(len(tokens))
word2vec_embeddings = [word2vec_model.wv[token] for token in tokens]

# Combine the BERT and Word2Vec embeddings
combined_embeddings = np.concatenate((bert_embeddings, np.mean(word2vec_embeddings, axis=0)))
