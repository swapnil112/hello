import lightgbm as lgb
import numpy as np

# Generate some random training data and labels
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, size=(100, 5))

# Define the LightGBM dataset
train_data = lgb.Dataset(X_train, label=y_train)

# Define the LightGBM parameters
params = {
    'objective': 'cross_entropy',
    'num_class': 5,
    'metric': 'multi_logloss'
}

# Train the LightGBM model
model = lgb.train(params, train_data)

# Make predictions on new data
X_new = np.random.rand(5, 10)
y_pred = model.predict(X_new)

# Threshold the predicted probabilities to get binary predictions
threshold = 0.5
y_pred_binary = (y_pred > threshold).astype(int)

print("Binary predictions:", y_pred_binary)





import lightgbm as lgb
import numpy as np

# Generate some random training data and labels
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, size=(100, 5))

# Train a separate LightGBM model for each label
models = []
for i in range(y_train.shape[1]):
    y = y_train[:, i]
    train_data = lgb.Dataset(X_train, label=y)
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss'
    }
    model = lgb.train(params, train_data)
    models.append(model)

# Make predictions on new data
X_new = np.random.rand(5, 10)
y_pred = np.zeros((X_new.shape[0], y_train.shape[1]))
for i, model in enumerate(models):
    y_pred[:, i] = model.predict(X_new)

# Threshold the predicted probabilities to get binary predictions
threshold = 0.5
y_pred_binary = (y_pred > threshold).astype(int)

print("Binary predictions:", y_pred_binary)






import xgboost as xgb
import numpy as np

# Generate some random training data and labels
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, size=(100, 5))

# Set up the XGBoost data matrix
dtrain = xgb.DMatrix(X_train, label=y_train)

# Train a separate XGBoost model for each label
models = []
for i in range(y_train.shape[1]):
    # Set up the XGBoost parameters
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'logloss',
        'num_round': 10
    }
    # Train the XGBoost model
    label_train = y_train[:, i]
    dtrain_label = xgb.DMatrix(X_train, label=label_train)
    model = xgb.train(params, dtrain_label)
    models.append(model)

# Make predictions on new data
X_new = np.random.rand(5, 10)
y_pred_prob = np.zeros((X_new.shape[0], y_train.shape[1]))
for i in range(y_train.shape[1]):
    dtest = xgb.DMatrix(X_new)
    y_pred_prob[:, i] = models[i].predict(dtest)

# Threshold the predicted probabilities to get binary predictions
threshold = 0.5
y_pred_binary = (y_pred_prob > threshold).astype(int)

print("Binary predictions:", y_pred_binary)




import xgboost as xgb
import numpy as np

# Generate some random training data and labels
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, size=(100, 5))

# Set up the XGBoost data matrix
dtrain = xgb.DMatrix(X_train, label=y_train)

# Set up the XGBoost parameters
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'num_round': 10
}

# Train the XGBoost model
model = xgb.train(params, dtrain)

# Make predictions on new data
X_new = np.random.rand(5, 10)
y_pred_prob = model.predict(xgb.DMatrix(X_new))

# Threshold the predicted probabilities to get binary predictions
threshold = 0.5
y_pred_binary = (y_pred_prob > threshold).astype(int)

print("Binary predictions:", y_pred_binary)


from sklearn.preprocessing import MultiLabelBinarizer
import pandas as pd

# Create a sample dataframe with a column containing lists of values
df = pd.DataFrame({'my_column': [[1, 2, 3], [2, 3], [4, 1, 2]]})

# Create a MultiLabelBinarizer object and fit it to the data
mlb = MultiLabelBinarizer()
mlb.fit(df['my_column'])

# Transform the column into a binary matrix representation
binary_matrix = mlb.transform(df['my_column'])

# Create a new dataframe with the binary matrix columns
binary_df = pd.DataFrame(binary_matrix, columns=mlb.classes_)

# Concatenate the new dataframe with the original dataframe
df = pd.concat([df, binary_df], axis=1)

# Drop the original column containing lists of values
df.drop('my_column', axis=1, inplace=True)

# Print the resulting dataframe
print(df)


from sklearn.preprocessing import MultiLabelBinarizer
import pandas as pd

# Create a sample dataframe with a column containing comma-separated string values
df = pd.DataFrame({'df_value': ['abc,def', 'def,abc,ghi']})

# Split the comma-separated string values into lists of individual values
df['df_value'] = df['df_value'].apply(lambda x: x.split(','))

# Create a MultiLabelBinarizer object and fit it to the data
mlb = MultiLabelBinarizer()
mlb.fit(df['df_value'])

# Transform the column into a binary matrix representation
binary_matrix = mlb.transform(df['df_value'])

# Create a new dataframe with the binary matrix columns
binary_df = pd.DataFrame(binary_matrix, columns=mlb.classes_)

# Concatenate the binary columns into a single column
#binary_concat = pd.Series(binary_df.values.tolist()).apply(lambda x: ','.join(map(str, x)))
binary_concat = pd.Series(binary_df.values.tolist()).apply(lambda x: [int(i) for i in x])


# Add the concatenated binary column to the original dataframe
df['binary_concat'] = binary_concat

# Drop the original column containing comma-separated string values
df.drop('df_value', axis=1, inplace=True)

# Print the resulting dataframe
print(df)


import xgboost as xgb
import numpy as np
from sklearn.model_selection import train_test_split

# Generate some random data and labels
X = np.random.rand(100, 10)
y = np.random.randint(0, 2, size=(100, 5))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Set up the XGBoost data matrices
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set up the XGBoost parameters
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'num_round': 10
}

# Train the XGBoost model
model = xgb.train(params, dtrain)

# Make predictions on the test set
y_pred_prob = model.predict(dtest)

# Threshold the predicted probabilities to get binary predictions
threshold = 0.5
y_pred_binary = (y_pred_prob > threshold).astype(int)

print("Binary predictions:", y_pred_binary)


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# Assuming your predicted labels are stored in a NumPy array 'y_pred'
y_pred = np.array([[0, 1, 0, 1, 0],
                   [1, 0, 1, 0, 0],
                   [0, 1, 1, 0, 0],
                   [1, 0, 1, 0, 1],
                   [0, 0, 1, 1, 0],
                   [1, 1, 0, 0, 1],
                   [0, 0, 0, 1, 1],
                   [1, 1, 1, 0, 1],
                   [0, 1, 0, 0, 1],
                   [1, 0, 0, 1, 0]])

# Assuming the actual labels are stored in a NumPy array 'y_true'
y_true = np.array([[0, 1, 0, 1, 1],
                   [1, 0, 1, 0, 1],
                   [0, 1, 1, 0, 0],
                   [1, 0, 1, 0, 1],
                   [0, 0, 1, 1, 0],
                   [1, 1, 0, 0, 1],
                   [0, 0, 0, 1, 0],
                   [1, 1, 1, 0, 1],
                   [0, 1, 0, 0, 1],
                   [1, 0, 0, 1, 0]])

# Convert both y_pred and y_true to 1D arrays
y_pred_flat = y_pred.flatten()
y_true_flat = y_true.flatten()

# Compute accuracy
acc = accuracy_score(y_true_flat, y_pred_flat)
print("Accuracy:", acc)

# Compute precision
prec = precision_score(y_true_flat, y_pred_flat)
print("Precision:", prec)

# Compute recall
rec = recall_score(y_true_flat, y_pred_flat)
print("Recall:", rec)

# Compute F1 score
f1 = f1_score(y_true_flat, y_pred_flat)
print("F1 score:", f1)
